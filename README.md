# Barcelona Archives System

AI-powered historical archives assistant using **RAG (Retrieval Augmented Generation)** with Vue.js frontend, Python FastAPI backend, and Qdrant vector database.

## ğŸ¯ Overview

This system provides an intelligent chat interface to explore Barcelona's historical archives. It uses advanced AI techniques to retrieve relevant historical documents and generate contextually accurate responses.

## ğŸ—ï¸ Architecture

```
User Query â†’ Frontend (Vue.js)
    â†“
Backend (FastAPI)
    â†“
[RAG Pipeline]
    â”œâ”€â†’ CLIP Text Encoder
    â”œâ”€â†’ Qdrant Vector Search
    â”œâ”€â†’ Context Retrieval
    â””â”€â†’ LLM (OpenAI/Anthropic/Gemini)
    â†“
Response + Source Citations
```

## ğŸ“ Project Structure

```
Barcelona-archives-system/
â”œâ”€â”€ frontend/              # Vue.js + TailwindCSS frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ views/
â”‚   â”‚   â”‚   â”œâ”€â”€ Home.vue      # Chat interface with NotebookLM-style citations
â”‚   â”‚   â”‚   â””â”€â”€ Settings.vue  # AI model configuration
â”‚   â”‚   â”œâ”€â”€ App.vue           # Main app layout with sidebar
â”‚   â”‚   â””â”€â”€ main.js           # Entry point
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ backend/               # Python FastAPI backend with RAG
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py       # Chat endpoint with RAG
â”‚   â”‚   â”‚   â””â”€â”€ admin.py      # RAG status monitoring
â”‚   â”‚   â”œâ”€â”€ agent.py          # RAG orchestration with LangChain
â”‚   â”‚   â”œâ”€â”€ rag_retriever.py  # Qdrant retriever
â”‚   â”‚   â”œâ”€â”€ clip_handler.py   # Text embedding model
â”‚   â”‚   â””â”€â”€ vector_store.py   # Vector database interface
â”‚   â”œâ”€â”€ main.py               # FastAPI application
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ pipeline/              # Document processing service
â”‚   â”œâ”€â”€ main.py            # Document encoding & Qdrant ingestion
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ data/              # Document files directory
â”‚
â”œâ”€â”€ docker-compose.yml     # Multi-service orchestration
â””â”€â”€ .env                   # Environment configuration
```

## ğŸš€ Key Features

### AI & RAG
- âœ… **Retrieval Augmented Generation (RAG)** - Context-aware responses based on actual documents
- âœ… **Vector Search** - Semantic search using CLIP multilingual embeddings
- âœ… **Multiple AI Providers** - OpenAI, Anthropic Claude, Google Gemini support
- âœ… **Source Citations** - NotebookLM-style source references with relevance scores
- âœ… **LangChain Integration** - Structured prompt engineering and context management

### Frontend
- âœ… **Modern Chat UI** - Claude/ChatGPT-inspired interface
- âœ… **Source Cards** - Expandable document citations with previews
- âœ… **Relevance Indicators** - Visual progress bars showing match quality
- âœ… **Dark Theme** - Monochromatic design with #009639 primary green
- âœ… **Responsive Design** - Mobile-friendly layout

### Backend
- âœ… **FastAPI** - High-performance async API
- âœ… **Qdrant Vector DB** - Efficient similarity search
- âœ… **Document Pipeline** - Automated encoding and ingestion
- âœ… **Admin Endpoints** - RAG system monitoring
- âœ… **CORS Configured** - Secure cross-origin requests

### DevOps
- âœ… **Docker Compose** - Single-command deployment
- âœ… **Environment Variables** - Centralized configuration
- âœ… **Persistent Storage** - Qdrant data volumes
- âœ… **Hot Reload** - Development mode support

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose (recommended)
- OR: Node.js 20+, Python 3.11+, and Qdrant running locally

### Option 1: Docker (Recommended)

**1. Configure environment variables:**
```bash
# Edit .env file and add your AI API key
GOOGLE_API_KEY=your-key-here
# or
OPENAI_API_KEY=your-key-here
# or
ANTHROPIC_API_KEY=your-key-here
```

**2. Start all services:** (Development)

**1. Start Qdrant:**
```bash
docker run -p 6333:6333 -v qdrant_data:/qdrant/storage qdrant/qdrant
```

**2. Start Pipeline Service:**
```bash
cd pipeline
pip install -r requirements.txt
python main.py
```
This creates sample documents and encodes them into Qdrant.

**3. Start Backend:**
```bash
cd backend
python -m venv venv
venv\Scripts\activate  # Windows: venv\Scripts\activate | Linux/Mac: source venv/bin/activate
pip install -r requirements.txt
python main.py
```
Backend runs at: http://localhost:8000
ğŸ”§ Configuration

### Environment Variables (.env)

```bash
# AI Model Configuration
MODEL_PROVIDER=gemini              # Options: gemini, openai, anthropic
MODEL_NAME=gemini-2.5-flash        # Model variant
MODEL_TEMPERATURE=0.7              # Response creativity (0-1)

# API Keys (add at least one)
GOOGLE_API_KEY=your-key-here
OPENAI_API_KEY=your-key-here
ANTHROPIC_API_KEY=your-key-here

# Vector Database
QDRANT_HOST=qdrant                 # Docker: qdrant | Local: localhost
QDRANT_PORT=6333

# Application
APP_NAME=Barcelona Archives System
DEBUG=True
ALLOWED_ORIGINS=http://localhost,http://localhost:80,http://localhost:3000,http://localhost:3001
```

### Adding Documents

Place document files in `pipeline/data/` directory:
```bash
pipeline/data/
â”œâ”€â”€ historical_records.txt
â”œâ”€â”€ architectural_plans.txt
â””â”€â”€ your_document.txt
```

The pipeline will automatically process and encode them into Qdrant.

## ğŸ“Š Sample Archive Documents

The system includes 5 comprehensive sample documents:

1. **Historical Records 1900-1920** - Municipal records from early 20th century
2. **Architectural Plans** - Gothic Quarter buildings (1850-1900)
3. **Civil Registry** - Birth, marriage, death certificates (1920-1950)
4. **Trade Union Records** - Labor movement documentation (1880-1930)
5. **Photography Collection** - Historical photographs (1920-1960)

## ğŸ” How RAG Works

1. **User asks a question** â†’ "What architectural plans are available?"
2. **Query encoding** â†’ CLIP model converts question to vector
3. **Vector search** â†’ Qdrant finds top 3 similar documents
4. **Context building** â†’ Retrieved documents assembled as context
5. **LLM generation** â†’ AI generates response using retrieved context
6. **Response with sources** â†’ Answer + citations with relevance scores

## ğŸ“¡ API Endpoints

### Chat Endpoints
- `POST /api/chat` - Send message, receive RAG response with sources
- `GET /api/model/config` - Get current AI model configuration
- `POST /api/model/config` - Update AI model settings
- `GET /api/model/providers` - List available AI providers

### Admin Endpoints
- `GET /api/admin/rag-status` - Check Qdrant connection and vector counts
- `GET /health` - Health check
- `GET /docs` - Interactive API documentation

## ğŸ› ï¸ Technology Stack

**Frontend:**
- Vue.js 3 (Composition API)
- Vite 6
- TailwindCSS 3
- Axios
- Vue Router

**Backend:**
- FastAPI 0.115
- LangChain 0.3
- Qdrant Client 1.12
- SentenceTransformers (CLIP)
- PyTorch
- Uvicorn

**AI/ML:**
- LangChain (RAG orchestration)
- CLIP ViT-B-32 Multilingual (embeddings)
- OpenAI / Anthropic / Google Gemini (LLMs)
- Qdrant (vector database)

**DevOps:**
- Docker & Docker Compose
- Nginx (production frontend)
- Python 3.11
- Node.js 20

## ğŸ“ˆ Monitoring

**Check RAG system status:**
```bash
curl http://localhost:8000/api/admin/rag-status
```

**Access Qdrant dashboard:**
```
http://localhost:6333/dashboard
```

**View collection info:**
- Collection name: `barcelona_archives`
- Vector dimension: 512
- Distance metric: Cosine similarity

## ğŸ¨ Design System

- **Primary Color**: #009639 (Barcelona green)
- **Background**: Monochromatic dark (#000000 to #1a1a1a)
- **Typography**: System fonts for readability
- **Layout**: Sidebar navigation with chat-style interface
- **Citations**: NotebookLM-inspired source cards

## ğŸ“ Development Notes

- Frontend uses Vite proxy for API calls in development
- Backend includes CORS middleware for cross-origin requests
- Pipeline service runs continuously, monitoring for new documents
- Qdrant data persists in Docker volumes
- All services communicate via Docker network

## ğŸš§ Future Enhancements

- [ ] PDF and DOCX document support in pipeline
- [ ] Multi-language query support
- [ ] Document upload via web interface
- [ ] Authentication and user management
- [ ] Conversation history persistence
- [ ] Advanced filtering and search options
- [ ] Export conversations and citations
- [ ] Analytics dashboard

## ğŸ“„ License

MIT License - See individual component licenses for details.
npm run dev
```

Frontend runs at: http://localhost:3000

## Features

- âœ… Vue.js 3 with Composition API
- âœ… TailwindCSS with custom dark theme
- âœ… FastAPI backend with RESTful API
- âœ… Environment variable support (.env files)
- âœ… Docker & Docker Compose support
- âœ… CORS configuration
- âœ… Mock data for development
- âœ… Responsive design
- âœ… API documentation at /docs
- âœ… Production-ready Nginx configuration

## Environment Configuration

Both frontend and backend include `.env` and `.env.example` files for easy configuration. See individual README files in each directory for details.

## API Documentation

Interactive API documentation available at:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## Next Steps

1. Install dependencies in both frontend and backend
2. Start both servers
3. Access the application at http://localhost:3000
4. Integrate with a real database (PostgreSQL, MongoDB, etc.)
5. Add authentication and authorization
6. Implement file upload for archive documents
